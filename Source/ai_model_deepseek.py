from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# âœ… ëª¨ë¸ ì´ë¦„
model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"

# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16 if device.type == "cuda" else torch.float32
).to(device)

# âœ… í”„ë¡¬í”„íŠ¸ â†’ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(prompt: str):
    messages = [
        {"role": "user", "content": prompt}
    ]
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=512,
            temperature=0.6,
            top_p=0.8,
            eos_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)

# âœ… ì£¼ì„ ë‹¬ê¸° í•¨ìˆ˜
def annotate_code_with_comments(code: str):
    prompt = f"ë‹¤ìŒì€ íŒŒì´ì¬ ì½”ë“œì…ë‹ˆë‹¤. ê° ì¤„ì— ì£¼ì„ì„ ë‹¬ì•„ì£¼ì„¸ìš”. ë°˜ë³µëœ ë‹µë³€ì€ ì¶œë ¥í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.\n\n{code}"
    return generate_response(prompt)

# âœ… ì½”ë“œ ìˆ˜ì • í•¨ìˆ˜
def modify_code(code: str):
    prompt = f"ë‹¤ìŒì€ íŒŒì´ì¬ ì½”ë“œì…ë‹ˆë‹¤. ì½”ë“œì— ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ìˆ˜ì •í•´ì£¼ì„¸ìš”. ë°˜ë³µëœ ë‹µë³€ì€ ì¶œë ¥í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.\n\n{code}"
    return generate_response(prompt)

# âœ… ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ§  DeepSeek Coder 1.3B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    while True:
        print("\n'ìƒì„±', 'ìˆ˜ì •', 'ì£¼ì„', 'ì¢…ë£Œ' ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì‹œì˜¤")
        prompt = input("ğŸ’¬ Prompt: ")
        if prompt.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            break

        elif prompt.lower() in ["ìƒì„±"]:
            print('\nâœï¸ ì›í•˜ëŠ” ì½”ë“œ í˜•íƒœë¥¼ ì ì–´ì£¼ì„¸ìš”.')
            code_type = input()
            full_prompt = f"ë‹¤ìŒì€ ì›í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. ì ì ˆí•œ íŒŒì´ì¬ ì½”ë“œë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”. ë°˜ë³µëœ ë‹µë³€ì€ ì¶œë ¥í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.\n\n{code_type}"
            response = generate_response(full_prompt)

        elif prompt.lower() in ["ì£¼ì„"]:
            print('\nâœï¸ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ End Code ì…ë ¥)')
            code_lines = []
            while True:
                line = input()
                if line.strip() == "End Code":
                    break
                code_lines.append(line)
            user_code = "\n".join(code_lines)
            response = annotate_code_with_comments(user_code)

        elif prompt.lower() in ["ìˆ˜ì •"]:
            print('\nâœï¸ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ End Code ì…ë ¥)')
            code_lines = []
            while True:
                line = input()
                if line.strip() == "End Code":
                    break
                code_lines.append(line)
            user_code = "\n".join(code_lines)
            response = modify_code(user_code)

        else:
            print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
            response = "ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”."
        
        print("\nğŸ§  ë‹µë³€:\n", response)
