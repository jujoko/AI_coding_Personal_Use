from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# âœ… ëª¨ë¸ ì´ë¦„
model_name = "bigcode/starcoder2-3b"

# âœ… 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True
)

# âœ… 3. ëª¨ë¸ ë¡œë“œ (4bit, device_map ìë™ í• ë‹¹, CPU ì˜¤í”„ë¡œë“œ ê°€ëŠ¥)
#GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½ -> ì†ë„ ë” ë¹¨ë¼ì§.
device = "cuda"
if torch.cuda.is_available():
    device_map = "auto"
    max_memory = {0: "20GiB"}  # GPU 0ë²ˆì— ìµœëŒ€ 20GiB ì‚¬ìš© í—ˆìš©
    torch_dtype = torch.float16
else:
    device_map = {"": "cpu"}
    max_memory = {"cpu": "12GiB"}
    torch_dtype = torch.float32

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map=device_map,
    torch_dtype=torch_dtype,
    max_memory=max_memory
).to(device)

# âœ… 4. í…ŒìŠ¤íŠ¸ìš© ì‘ë‹µ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì…ë ¥ â†’ í…ìŠ¤íŠ¸ ì¶œë ¥)
def generate_response(prompt: str):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… 5. ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ§  StarCoder2-3B model loaded. Type 'exit' to quit.")
    while True:
        prompt = input("ğŸ’¬ Prompt: ")
        if prompt.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            break
        response = generate_response(prompt)
        print("\nğŸ§  ë‹µë³€:\n", response)